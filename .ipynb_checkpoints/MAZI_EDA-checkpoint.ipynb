{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefb591d-6588-4007-b648-ba90f3c80c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os ; from glob import glob\n",
    "import fnmatch\n",
    "import math\n",
    "import numpy as np ; import pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "import xlsxwriter\n",
    "from matplotlib import pyplot as plt\n",
    "from pykalman import KalmanFilter\n",
    "from scipy import stats\n",
    "import pywt\n",
    "from skimage.restoration import denoise_wavelet\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tqdm.notebook import tqdm ; import seaborn as sns\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfb0337c-d15c-43bd-b9f2-307f1913f1aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터프레임 만들기\n",
    "def make_dataframe(train_path, val_path):\n",
    "    train_test_set = sorted(glob(train_path + '/*.csv'))\n",
    "    validation_set = sorted(glob(val_path + '/*.csv'))\n",
    "    df_98_temp     = pd.DataFrame()\n",
    "    df_35_temp     = pd.DataFrame()\n",
    "    \n",
    "    category = 0\n",
    "    for i,file in enumerate(train_test_set):\n",
    "        user_df_98 = pd.read_csv(file)\n",
    "        user_df_98 = user_df_98[user_df_98.columns[:22]][:]\n",
    "        \n",
    "        lst_730   = user_df_98.columns[1:8].tolist()\n",
    "        lst_850   = user_df_98.columns[8:15].tolist()\n",
    "        lst_back = user_df_98.columns[15:22].tolist()\n",
    "        for idx in range(7):\n",
    "            user_df_98[lst_730[idx]] = user_df_98[lst_730[idx]] - np.mean(user_df_98[lst_730[idx]]) #  DC OFFSET 제거\n",
    "            user_df_98[lst_850[idx]] = user_df_98[lst_850[idx]] - np.mean(user_df_98[lst_850[idx]]) #  DC OFFSET 제거\n",
    "            user_df_98[lst_730[idx]] = user_df_98[lst_730[idx]] + user_df_98[lst_back[idx]] + 1300\n",
    "            user_df_98[lst_850[idx]] = user_df_98[lst_850[idx]] + user_df_98[lst_back[idx]] + 1300\n",
    "        user_df_98 = user_df_98[user_df_98.columns[:15]][:]\n",
    "        user_id    = int(file.split('/')[-1].split('\\\\')[1].split(\".\")[0])\n",
    "        user_df_98.insert(0,'uid',user_id)\n",
    "        if i<55:\n",
    "            category='CN'\n",
    "        elif 55<=i<=85:\n",
    "            category='MCI'\n",
    "        elif 85<=i<=99:\n",
    "            category='AD'\n",
    "        user_df_98['Category'] = category\n",
    "        df_98_temp             = pd.concat([df_98_temp,user_df_98])\n",
    "        \n",
    "    uid_lst = list(df_98_temp.uid.unique().tolist())\n",
    "    df_98   = pd.DataFrame()\n",
    "    for i in uid_lst:\n",
    "        df    = df_98_temp[df_98_temp.uid==i]\n",
    "        df_98 = pd.concat([df_98,df])\n",
    "    df_98 = df_98.reset_index(drop=True)\n",
    "    del df_98_temp\n",
    "\n",
    "    for i, file in enumerate(validation_set):\n",
    "        user_df_35 = pd.read_csv(file)\n",
    "        user_df_35 = user_df_35[user_df_35.columns[:22]][:]\n",
    "    \n",
    "        lst_730  = user_df_35.columns[1:8].tolist()\n",
    "        lst_850  = user_df_35.columns[8:15].tolist()\n",
    "        lst_back = user_df_35.columns[15:22].tolist()\n",
    "    \n",
    "        for idx in range(7) :\n",
    "            user_df_35[lst_730[idx]] = user_df_35[lst_730[idx]] - np.mean(user_df_35[lst_730[idx]]) #  DC OFFSET 제거\n",
    "            user_df_35[lst_850[idx]] = user_df_35[lst_850[idx]] - np.mean(user_df_35[lst_850[idx]]) #  DC OFFSET 제거\n",
    "            user_df_35[lst_730[idx]] = user_df_35[lst_730[idx]] + user_df_35[lst_back[idx]] + 200\n",
    "            user_df_35[lst_850[idx]] = user_df_35[lst_850[idx]] + user_df_35[lst_back[idx]] + 200\n",
    "        user_df_35 = user_df_35[user_df_35.columns[:15]][:]\n",
    "        uid        = int(file.split('.')[0].split('/')[-1].split('\\\\')[-1])\n",
    "        user_df_35.insert(0,'uid', uid)\n",
    "        if i<=5:\n",
    "            category = 'AD'\n",
    "        elif 6<=i<=21:\n",
    "            category = 'CN'\n",
    "        elif 22<=i<=36:\n",
    "            category = \"MCI\"\n",
    "        user_df_35['Category'] = category\n",
    "        df_35_temp = pd.concat([df_35_temp,user_df_35])\n",
    "    \n",
    "    uid_lst = list(set(df_35_temp.uid.unique().tolist()))\n",
    "    df_35   = pd.DataFrame()\n",
    "    for uid in uid_lst :\n",
    "        df    = df_35_temp[df_35_temp.uid==uid]\n",
    "        df_35 = pd.concat([df_35,df])\n",
    "    \n",
    "    df_35 = df_35.reset_index(drop = True)\n",
    "    del df_35_temp\n",
    "    def all_section_slice_data(df, ss_1,es_1,ss_2,es_2,ss_3,es_3,ss_4,es_4,ss_5,es_5) :\n",
    "        user_lst = df.uid.unique()\n",
    "        new_df   = pd.DataFrame()\n",
    "        for i in user_lst:\n",
    "            slice_df = pd.concat([df[df.uid==i][ss_1:es_1],df[df.uid==i][ss_2:es_2],df[df.uid==i][ss_3:es_3]\n",
    "                                  ,df[df.uid==i][ss_4:es_4],df[df.uid==i][ss_5:es_5]])\n",
    "            new_df   = pd.concat([new_df,slice_df])\n",
    "        return new_df\n",
    "    \n",
    "    ss_1,es_1,ss_2,es_2,ss_3,es_3,ss_4,es_4,ss_5,es_5 = 0,1000,1400,2000,2400,3000,3400,4000,4400,5000\n",
    "    df_35 = all_section_slice_data(df_35,\n",
    "                                   ss_1,es_1,ss_2,es_2,ss_3,es_3,ss_4,es_4,ss_5,es_5).reset_index(drop=True)\n",
    "    return df_98,df_35,train_test_set,validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31584b93-7b5d-4a74-847f-5bf103ad68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러온 데이터 프레임 전처리하지 않고 그래프로 plot\n",
    "def raw_data_plot(df, name, uids, channel):\n",
    "    print('\\n특정 유저 특징 채널 불러오기')\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title(f'{name}명 Raw Data - User ID: {uids} - 730nm,850nm - {channel}channel')\n",
    "    user_data = df[df.uid == uids]\n",
    "    user_data[f'A_730_{channel}'].plot(label=f'730nm Channel {channel}')\n",
    "    user_data[f'A_850_{channel}'].plot(label=f'850nm Channel {channel}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad19e39d-a7eb-4f2d-b43b-fbd6e6c22267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot\n",
    "def data_boxplot(df):\n",
    "    df[df.columns[2:]].boxplot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b6cfd7-ed05-4923-87da-44e95752135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리를 boxplot\n",
    "def category_boxplot(df):\n",
    "    for i in range(7):\n",
    "        sns.boxplot(data=df, y=f'A_730_{i}', x='Category')\n",
    "        sns.boxplot(data=df, y=f'A_850_{i}', x='Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "807a4b62-9b6d-4211-99b3-e19fc1dc9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_heatmap(df):\n",
    "    df_ = df.copy()\n",
    "    df_['Category_2'] = 0\n",
    "    df_.loc[df_['Category'] == 'CN' , 'Category_2'] = 0\n",
    "    df_.loc[df_['Category'] == 'MCI' , 'Category_2'] = 1\n",
    "    df_.loc[df_['Category'] == 'AD' , 'Category_2'] = 2\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(data = df_[df_.columns[2:]].corr(), annot=True,\n",
    "                fmt = '.2f', linewidths=.5, cmap='Blues')\n",
    "    plt.savefig('heamap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76eeeab6-b9a3-4fdb-ab18-5e434801432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 별 이상치\n",
    "def scatter_per_category(df, wave):\n",
    "    for i in range(7):\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        df.groupby('Index').plot(kind='scatter', x='Category', y=f'A_{wave}_{i}', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ed04911-9da9-4594-8c22-c6c4e8c7ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel 별 이상치\n",
    "def outlier_iqr(data, column, uids):\n",
    "    q25, q75 = np.quantile(data[column], 0.25), np.quantile(data[column], 0.75)\n",
    "    iqr      = q75 - q25\n",
    "    cut_off  = iqr * 1.5\n",
    "\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "    \n",
    "    print('IQR은', iqr, '이다.')\n",
    "    print('lower bound 값은', lower, '이다.')\n",
    "    print('upper bound 값은', upper, '이다.')\n",
    "\n",
    "    data1 = data[data[column] > upper]\n",
    "    data2 = data[data[column] < lower]\n",
    "\n",
    "    print(f'{column} 컬럼의 총 이상치 건수는 {data1.shape[0] + data2.shape[0]} 이다.')\n",
    "    print(f'{column} 컬럼에 대한 모든 유저의 이상치를 출력합니다.')\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.distplot(data[column], kde=False)\n",
    "\n",
    "    plt.axvspan(xmin=lower, xmax=data[column].min(), alpha=0.5, color='red')\n",
    "    plt.axvspan(xmin=upper, xmax=data[column].max(), alpha=0.5, color='red')\n",
    "    plt.title(f'모든 유저의 {column} 컬럼 이상치')\n",
    "    plt.savefig('all_user_columns.png')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'\\n이상치가 존재하는 유저 목록 : {data1.uid.unique().tolist()}')\n",
    "    print(f'{uids}번 유저의 이상치 구간(실선은 제외) | 이상치 건수 : {len(data1[data1.uid==uids])} (주황색 그래프)')\n",
    "    print(f'{column} 컬럼의 {uids} 유저의 이상치를 출력합니다.')\n",
    "\n",
    "    user_data  = data[data.uid==uids]\n",
    "    data1_data = data1[data1.uid==uids]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    user_data[f'{column}'].plot()\n",
    "    data1_data[f'{column}'].plot()\n",
    "    plt.title(f'{uids}번 유저의 {column} 컬럼 이상치')\n",
    "    plt.savefig('n_user_columns.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01b3df18-bff3-495c-87fe-dbbffe62f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_lowpass_filter(data, cutoff, fs, order):\n",
    "    normal_cutoff = 2 * cutoff/fs\n",
    "    # Get the filter coefficients\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    y = filtfilt(b,a,data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e2b7d04-28ad-4a93-8e0d-89dfb055d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_filter(y):\n",
    "    y = np.ma.array(y)\n",
    "    dt = 0.1\n",
    "    transpose_matrix = np.array([[1,dt],[0,1]])\n",
    "    observation_matrix = np.asarray([[1, 0]])\n",
    "    kf1 = KalmanFilter(transition_matrices = transpose_matrix, observation_matrices = observation_matrix)\n",
    "\n",
    "    kf1 = kf1.em(y)\n",
    "\n",
    "    (smoothed_state_means, _) = kf1.smooth(y)\n",
    "    return smoothed_state_means[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6666952-831c-4efb-a6d7-2d91fae3fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_convert_butterworth(df1, df2):\n",
    "    wave_df1 = pd.DataFrame()\n",
    "    wave_df2 = pd.DataFrame()\n",
    "\n",
    "    for idx in tqdm(df1.uid.unique()):\n",
    "        uid1    = df1[df1.uid==idx]\n",
    "        col_lst1 = uid1.columns[2:-1].tolist()\n",
    "        for i in col_lst1:\n",
    "            uid1[i] = denoise_wavelet(uid1[i].values, wavelet='db16', mode='soft',\n",
    "                                       wavelet_levels=5, method='BayesShrink', rescale_sigma='True')\n",
    "            uid1[i] = butter_lowpass_filter(uid1[i].values, 0.15, 3, 5)\n",
    "        wave_df1 = pd.concat([wave_df1, uid1])\n",
    "    wave_df1 = wave_df1.reset_index(drop=True)\n",
    "\n",
    "    for idx in tqdm(df2.uid.unique()):\n",
    "        uid2    = df2[df2.uid==idx]\n",
    "        col_lst2 = uid2.columns[2:-1].tolist()\n",
    "        for j in col_lst2:\n",
    "            uid2[j] = denoise_wavelet(uid2[j].values, wavelet='db16', mode='soft',\n",
    "                                       wavelet_levels=5, method='BayesShrink', rescale_sigma='True')\n",
    "            uid2[j] = butter_lowpass_filter(uid2[j].values, 0.15, 3, 5)\n",
    "        wave_df2 = pd.concat([wave_df2, uid2])\n",
    "    wave_df2 = wave_df2.reset_index(drop=True)\n",
    "    return wave_df1, wave_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "247e46d3-50ce-448d-81a6-2caa0a230156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_convert_kalman(df1, df2):\n",
    "    wave_df1 = pd.DataFrame()\n",
    "    wave_df2 = pd.DataFrame()\n",
    "\n",
    "    for idx in tqdm(df1.uid.unique()):\n",
    "        uid1    = df1[df1.uid==idx]\n",
    "        col_lst1 = uid1.columns[2:-1].tolist()\n",
    "        for i in col_lst1:\n",
    "            uid1[i] = kalman_filter(uid1[i].values)\n",
    "        wave_df1 = pd.concat([wave_df1, uid1])\n",
    "    wave_df1 = wave_df1.reset_index(drop=True)\n",
    "\n",
    "    for idx in tqdm(df2.uid.unique()):\n",
    "        uid2    = df2[df2.uid==idx]\n",
    "        col_lst2 = uid2.columns[2:-1].tolist()\n",
    "        for j in col_lst2:\n",
    "            uid2[j] = kalman_filter(uid2[j].values)\n",
    "        wave_df2 = pd.concat([wave_df2, uid2])\n",
    "    wave_df2 = wave_df2.reset_index(drop=True)\n",
    "    return wave_df1, wave_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7b68df5-1a30-4104-9c81-deab413e5713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_convert_gaussian(df1, df2):\n",
    "    wave_df1 = pd.DataFrame()\n",
    "    wave_df2 = pd.DataFrame()\n",
    "\n",
    "    for idx in tqdm(df1.uid.unique()):\n",
    "        uid1    = df1[df1.uid==idx]\n",
    "        col_lst1 = uid1.columns[2:-1].tolist()\n",
    "        for i in col_lst1:\n",
    "            uid1[i] = gaussian_filter1d(uid1[i].values, 3)\n",
    "        wave_df1 = pd.concat([wave_df1, uid1])\n",
    "    wave_df1 = wave_df1.reset_index(drop=True)\n",
    "\n",
    "    for idx in tqdm(df2.uid.unique()):\n",
    "        uid2    = df2[df2.uid==idx]\n",
    "        col_lst2 = uid2.columns[2:-1].tolist()\n",
    "        for j in col_lst2:\n",
    "            uid2[j] = gaussian_filter1d(uid2[j].values, 3)\n",
    "        wave_df2 = pd.concat([wave_df2, uid2])\n",
    "    wave_df2 = wave_df2.reset_index(drop=True)\n",
    "    return wave_df1, wave_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aca6f39-46da-41d8-a291-c606b1676aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_plot(df, name, uids, channel):\n",
    "    plt.title(f'{name}명 wavelet denoise - User name : {uids} - 730nm, 850nm - {channel} channel')\n",
    "    wave_df = df[df.uid==uids]\n",
    "    wave_df[f'A_730_{channel}'].plot(figsize=(10, 5))\n",
    "    wave_df[f'A_850_{channel}'].plot(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aff501b-6506-45a4-983f-89befd86646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def od_convert(wav1, wav2, start):\n",
    "    od_wav1 = pd.DataFrame()\n",
    "    od_wav2 = pd.DataFrame()\n",
    "\n",
    "    for idx in tqdm(wav1.uid.unique()):\n",
    "        raw    = wav1[wav1.uid==idx].reset_index(drop=True)\n",
    "        lst730 = raw.columns[2:9].tolist()\n",
    "        for jdx in lst730:\n",
    "            ch         = jdx.split(\"_\")[-1]\n",
    "            ch730_name = f'A_730_{ch}'   ; ch850_name = f'A_850_{ch}'\n",
    "            data730    = raw[ch730_name] ; data850    = raw[ch850_name]\n",
    "            \n",
    "            base730    = data730.copy()  ; base850    = data850.copy()\n",
    "            base730[base730.index > start] = np.mean(base730[:start])\n",
    "            base850[base850.index > start] = np.mean(base850[:start])\n",
    "            for i in base730.index[:start].tolist():\n",
    "                base730[i] = float(np.mean(data730[0:i+1]))\n",
    "            for j in base850.index[:start].tolist():\n",
    "                base850[j] = float(np.mean(data850[0:j+1]))\n",
    "\n",
    "            raw[ch730_name] = np.log10(base730/data730)\n",
    "            raw[ch850_name] = np.log10(base850/data850)\n",
    "        od_wav1 = pd.concat([od_wav1, raw])\n",
    "    od_wav1 = od_wav1.reset_index(drop=True)\n",
    "    \n",
    "    for kdx in tqdm(wav2.uid.unique()):\n",
    "        raw    = wav2[wav2.uid==kdx].reset_index(drop=True)\n",
    "        lst730 = raw.columns[2:9].tolist()\n",
    "        lst850 = raw.columns[9:16].tolist()\n",
    "        for ldx in lst730:\n",
    "            ch         = ldx.split(\"_\")[-1]\n",
    "            ch730_name = f'A_730_{ch}'   ; ch850_name = f'A_850_{ch}'\n",
    "            data730    = raw[ch730_name] ; data850    = raw[ch850_name]\n",
    "            \n",
    "            base730    = data730.copy()  ; base850    = data850.copy()\n",
    "            base730[base730.index > start] = np.mean(base730[:start])\n",
    "            base730[base850.index > start] = np.mean(base850[:start])\n",
    "            for i in base730.index[:start].tolist():\n",
    "                base730[i] = float(np.mean(data730[0:i+1]))\n",
    "            for j in base850.index[:start].tolist():\n",
    "                base850[j] = float(np.mean(data850[0:j+1]))\n",
    "\n",
    "            raw[ch730_name] = np.log10(base730/data730)\n",
    "            raw[ch850_name] = np.log10(base850/data850)\n",
    "        od_wav2 = pd.concat([od_wav2, raw])\n",
    "    od_wav2 = od_wav2.reset_index(drop=True)\n",
    "    return od_wav1, od_wav2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dac9f08-5a55-437e-b2fd-2bbdf37591f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def od_plot(od_wav, name, uids, channel):\n",
    "    plt.title(f'{name}명 optical density - User name : {uids} - 730nm, 850nm - {channel} channel')\n",
    "    od_wav = od_wav[od_wav.uid==uids]\n",
    "    od_wav[f'A_730_{channel}'].plot(figsize=(10, 5))\n",
    "    od_wav[f'A_850_{channel}'].plot(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f136b680-1cd5-4564-a398-40fa696d3c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hc_convert(od_wav1, od_wav2):\n",
    "    # e730_oxy   = 0.1096 * 4\n",
    "    # e730_deoxy = 0.3257 * 4\n",
    "    # e850_oxy   = 0.2899 * 4\n",
    "    # e850_deoxy = 0.1965 * 4\n",
    "    e730_oxy   = 2.1838\n",
    "    e730_deoxy = 7.0146*1.355\n",
    "    e850_oxy   = 10.2470\n",
    "    e850_deoxy = 6.8501*1.355\n",
    "    e_c = (e730_oxy*e850_deoxy-e850_oxy*e730_deoxy)\n",
    "    \n",
    "    separation_730 = np.array([4, 3.5, 3, 3.5, 4, 3, 1])\n",
    "    separation_850 = np.array([4, 3.5, 3, 3.5, 4, 3, 1])\n",
    "    \n",
    "    hb_df1 = pd.DataFrame()\n",
    "    for idx in od_wav1.uid.unique():\n",
    "    \n",
    "        od = od_wav1[od_wav1.uid==idx].reset_index(drop=True)\n",
    "    \n",
    "        lst_730 = od.columns[2:9].tolist()\n",
    "        for jdx in lst_730 :\n",
    "            ch         = jdx.split(\"_\")[-1]\n",
    "            ch730_name = f'A_730_{ch}' ; ch850_name = f'A_850_{ch}'\n",
    "    \n",
    "            hbo_name = f'hbo_{str(int(ch)+1)}' ; hbr_name = f'hbr_{str(int(ch)+1)}'\n",
    "    \n",
    "            OD730 = od[ch730_name] ; OD850 = od[ch850_name]\n",
    "    \n",
    "            hbo = (OD730*e850_deoxy-OD850*e730_deoxy)/(e_c * separation_850[int(ch)])\n",
    "            hbr = (OD850*e730_oxy-OD730*e850_oxy)/(e_c * separation_850[int(ch)])\n",
    "            od[ch730_name] = hbo\n",
    "            od[ch850_name] = hbr\n",
    "    \n",
    "            od.rename(columns = {ch730_name:hbo_name, ch850_name:hbr_name}, inplace = True )\n",
    "        hb_df1 = pd.concat([hb_df1,od])\n",
    "    hb_df1 = hb_df1.reset_index(drop=True)\n",
    "\n",
    "    hb_df2 = pd.DataFrame()\n",
    "    for idx in od_wav2.uid.unique():\n",
    "    \n",
    "        od = od_wav2[od_wav2.uid==idx].reset_index(drop=True)\n",
    "    \n",
    "        lst_730 = od.columns[2:9].tolist()\n",
    "        for jdx in lst_730 :\n",
    "            ch         = jdx.split(\"_\")[-1]\n",
    "            ch730_name = f'A_730_{ch}' ; ch850_name = f'A_850_{ch}'\n",
    "    \n",
    "            hbo_name = f'hbo_{str(int(ch)+1)}' ; hbr_name = f'hbr_{str(int(ch)+1)}'\n",
    "    \n",
    "            OD730 = od[ch730_name] ; OD850 = od[ch850_name]\n",
    "    \n",
    "            hbo = (OD730*e850_deoxy-OD850*e730_deoxy)/(e_c * separation_850[int(ch)])\n",
    "            hbr = (OD850*e730_oxy-OD730*e850_oxy)/(e_c * separation_850[int(ch)])\n",
    "            od[ch730_name] = hbo\n",
    "            od[ch850_name] = hbr\n",
    "    \n",
    "            od.rename(columns = {ch730_name:hbo_name, ch850_name:hbr_name}, inplace = True )\n",
    "        hb_df2 = pd.concat([hb_df2,od])\n",
    "    hb_df2 = hb_df2.reset_index(drop=True)\n",
    "    return hb_df1, hb_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14edc0a3-6396-4e9e-86e7-7e33e82e4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hb_plot(hb_wav, name, uids, channel):\n",
    "    plt.title(f'{name}명 HB data - User name : {uids} - 730nm, 850nm - {channel} channel')\n",
    "    hb_wav = hb_wav[hb_wav.uid==uids]\n",
    "    hb_wav[f'hbo_{channel}'].plot(figsize=(10, 5))\n",
    "    hb_wav[f'hbr_{channel}'].plot(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91d96275-578f-4fe1-8a3f-5dbaff18610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chb_convert(hb1, hb2):\n",
    "    chb_df_98 = pd.DataFrame()\n",
    "    for idx in hb1.uid.unique() :\n",
    "        chb_df = pd.DataFrame()\n",
    "        uid1  = hb1[hb1.uid==idx]\n",
    "        hbo_lst  = uid1[uid1.columns[2:8]].columns\n",
    "        hbr_lst  = uid1[uid1.columns[9:15]].columns\n",
    "        for jdx in range(len(hbo_lst)) :\n",
    "            cNIRS_HbO_constant = np.dot(uid1[hbo_lst[jdx]], uid1.hbo_7) / np.dot(uid1[hbo_lst[jdx]], uid1.hbo_7)\n",
    "            cNIRS_Hb_constant  = np.dot(uid1[hbr_lst[jdx]], uid1.hbr_7) / np.dot(uid1[hbr_lst[jdx]], uid1.hbr_7)\n",
    "            if cNIRS_HbO_constant>0:\n",
    "                cHbO = uid1[hbo_lst[jdx]] - uid1.hbo_7 * float(cNIRS_HbO_constant)\n",
    "            else:\n",
    "                cHbO = uid1[hbo_lst[jdx]]\n",
    "            if cNIRS_Hb_constant >0:\n",
    "                cHb = uid1[hbr_lst[jdx]] - uid1.hbr_7 * float(cNIRS_Hb_constant)\n",
    "            else:\n",
    "                cHb = uid1[hbr_lst[jdx]]\n",
    "            chb_df['c' + hbo_lst[jdx]] = cHbO[:3400]\n",
    "            chb_df['c' + hbr_lst[jdx]] = cHb[:3400]\n",
    "        chb_df.insert(0,'uid', idx)\n",
    "        chb_df = chb_df[['uid','chbo_1','chbo_2','chbo_3','chbo_4','chbo_5','chbo_6'\n",
    "                         ,'chbr_1','chbr_2','chbr_3','chbr_4','chbr_5','chbr_6']]\n",
    "        chb_df['Category'] = uid1.Category.unique()[0]\n",
    "        chb_df_98 = pd.concat([chb_df_98, chb_df])\n",
    "    \n",
    "    chb_df_98 = chb_df_98.reset_index(drop=True)\n",
    "    pd1= chb_df_98.chbo_1 - chb_df_98.chbr_1\n",
    "    pd2= chb_df_98.chbo_2 - chb_df_98.chbr_2\n",
    "    pd3= chb_df_98.chbo_3 - chb_df_98.chbr_3\n",
    "    pd4= chb_df_98.chbo_4 - chb_df_98.chbr_4\n",
    "    pd5= chb_df_98.chbo_5 - chb_df_98.chbr_5\n",
    "    pd6= chb_df_98.chbo_6 - chb_df_98.chbr_6\n",
    "    \n",
    "    lr01 = (pd1 + pd2 + pd3) - (pd4 + pd5 + pd6)\n",
    "    chb_df_98.insert(13, 'lr01', lr01)\n",
    "    chb_df_98.insert(14, 'dc_lr01', lr01 - np.mean(lr01))  # remove DC OFFSET\n",
    "    chb_df_98.insert(15, 'mean_chb', (pd1 + pd2 + pd3 +pd4 + pd5 + pd6) / 6)\n",
    "\n",
    "    chb_df_35 = pd.DataFrame()\n",
    "    for idx in hb2.uid.unique() :\n",
    "        chb_df = pd.DataFrame()\n",
    "        uid1  = hb2[hb2.uid==idx]\n",
    "        hbo_lst  = uid1[uid1.columns[2:8]].columns\n",
    "        hbr_lst  = uid1[uid1.columns[9:15]].columns\n",
    "        for jdx in range(len(hbo_lst)) :\n",
    "            cNIRS_HbO_constant = np.dot(uid1[hbo_lst[jdx]], uid1.hbo_7) / np.dot(uid1[hbo_lst[jdx]], uid1.hbo_7)\n",
    "            cNIRS_Hb_constant  = np.dot(uid1[hbr_lst[jdx]], uid1.hbr_7) / np.dot(uid1[hbr_lst[jdx]], uid1.hbr_7)\n",
    "            if cNIRS_HbO_constant>0:\n",
    "                cHbO = uid1[hbo_lst[jdx]] - uid1.hbo_7 * float(cNIRS_HbO_constant)\n",
    "            else:\n",
    "                cHbO = uid1[hbo_lst[jdx]]\n",
    "            if cNIRS_Hb_constant >0:\n",
    "                cHb = uid1[hbr_lst[jdx]] - uid1.hbr_7 * float(cNIRS_Hb_constant)\n",
    "            else:\n",
    "                cHb = uid1[hbr_lst[jdx]]\n",
    "            chb_df['c' + hbo_lst[jdx]] = cHbO[:3400]\n",
    "            chb_df['c' + hbr_lst[jdx]] = cHb[:3400]\n",
    "    \n",
    "        chb_df.insert(0,'uid', idx)\n",
    "        chb_df = chb_df[['uid','chbo_1','chbo_2','chbo_3','chbo_4','chbo_5','chbo_6'\n",
    "                         ,'chbr_1','chbr_2','chbr_3','chbr_4','chbr_5','chbr_6']]\n",
    "        chb_df['Category'] = uid1.Category.unique()[0]\n",
    "        chb_df_35 = pd.concat([chb_df_35, chb_df])\n",
    "    \n",
    "    chb_df_35 = chb_df_35.reset_index(drop=True)\n",
    "    pd1= chb_df_35.chbo_1 - chb_df_35.chbr_1\n",
    "    pd2= chb_df_35.chbo_2 - chb_df_35.chbr_2\n",
    "    pd3= chb_df_35.chbo_3 - chb_df_35.chbr_3\n",
    "    pd4= chb_df_35.chbo_4 - chb_df_35.chbr_4\n",
    "    pd5= chb_df_35.chbo_5 - chb_df_35.chbr_5\n",
    "    pd6= chb_df_35.chbo_6 - chb_df_35.chbr_6\n",
    "    \n",
    "    lr01 = (pd1 + pd2 + pd3) - (pd4 + pd5 + pd6)\n",
    "    chb_df_35.insert(13, 'lr01', lr01)\n",
    "    chb_df_35.insert(14, 'dc_lr01', lr01 - np.mean(lr01))  # remove DC OFFSET\n",
    "    chb_df_35.insert(15, 'mean_chb', (pd1 + pd2 + pd3 +pd4 + pd5 + pd6) / 6)\n",
    "\n",
    "    return chb_df_98, chb_df_35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcbfe29e-11ce-451b-a41d-dfdcd611c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chb_plot(df, name, uids, channel):\n",
    "    plt.title(f'{name}명 cHB data - User name : {uids} - 730nm, 850nm - {channel} channel')\n",
    "    chb_df = df[df.uid==uids]\n",
    "    chb_df[f'chbo_{channel}'].plot(figsize=(10, 5))\n",
    "    chb_df[f'chbr_{channel}'].plot(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794610b-afbc-45af-8fad-0e2763a72854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f64e1a-923b-4775-a3ed-eb8fff6fc6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd01877-b7f1-437a-88bb-0dc1632dc854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
